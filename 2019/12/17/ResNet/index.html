<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="残差网络,">










<meta name="description" content="残差网络  ResNet全名Residual Network残差网络。Kaiming He 的《Deep Residual Learning for Image Recognition》获得了CVPR最佳论文。  随着网络深度的不断增加，人们发现深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，test dataset的分类准确率也变得更差。">
<meta name="keywords" content="残差网络">
<meta property="og:type" content="article">
<meta property="og:title" content="ResNet残差网络">
<meta property="og:url" content="https://xiaohaoke.top/2019/12/17/ResNet/index.html">
<meta property="og:site_name" content="xiaohaoke&#39;s blog">
<meta property="og:description" content="残差网络  ResNet全名Residual Network残差网络。Kaiming He 的《Deep Residual Learning for Image Recognition》获得了CVPR最佳论文。  随着网络深度的不断增加，人们发现深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，test dataset的分类准确率也变得更差。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https:////upload-images.jianshu.io/upload_images/11753914-27655f0d9960084e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200">
<meta property="og:image" content="https:////upload-images.jianshu.io/upload_images/11753914-95c14f704c7fc204.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/640">
<meta property="og:image" content="https://math.jianshu.com/math?formula=H(x">
<meta property="og:image" content="https://math.jianshu.com/math?formula=F(x">
<meta property="og:image" content="https://math.jianshu.com/math?formula=F(x">
<meta property="og:image" content="https://math.jianshu.com/math?formula=H(x">
<meta property="og:image" content="https:////upload-images.jianshu.io/upload_images/11753914-ab6bad517b3b2ae4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/421">
<meta property="og:image" content="https:////upload-images.jianshu.io/upload_images/11753914-f07cf65c67b50bdd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/844">
<meta property="og:image" content="https:////upload-images.jianshu.io/upload_images/11753914-2e266cfa660a8a74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/384">
<meta property="og:image" content="https:////upload-images.jianshu.io/upload_images/11753914-4c3348bd881bd4e6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1092">
<meta property="og:image" content="https://math.jianshu.com/math?formula=y%20%3D%20F(x">
<meta property="og:image" content="https://math.jianshu.com/math?formula=y%20%3D%20F(x">
<meta property="og:image" content="https://math.jianshu.com/math?formula=W">
<meta property="og:image" content="https:////upload-images.jianshu.io/upload_images/11753914-0c9b9b79e98c7731.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200">
<meta property="og:image" content="https:////upload-images.jianshu.io/upload_images/11753914-5ce18da629e4ae1a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200">
<meta property="og:updated_time" content="2019-12-17T11:35:25.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ResNet残差网络">
<meta name="twitter:description" content="残差网络  ResNet全名Residual Network残差网络。Kaiming He 的《Deep Residual Learning for Image Recognition》获得了CVPR最佳论文。  随着网络深度的不断增加，人们发现深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，test dataset的分类准确率也变得更差。">
<meta name="twitter:image" content="https:////upload-images.jianshu.io/upload_images/11753914-27655f0d9960084e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://xiaohaoke.top/2019/12/17/ResNet/">





  <title>ResNet残差网络 | xiaohaoke's blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">xiaohaoke's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://xiaohaoke.top/2019/12/17/ResNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="xiaohaoke">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/uploads/logo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="xiaohaoke's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">ResNet残差网络</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-12-17T19:33:56+08:00">
                2019-12-17
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/残差网络/" itemprop="url" rel="index">
                    <span itemprop="name">残差网络</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/12/17/ResNet/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/12/17/ResNet/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h2><p>  ResNet全名Residual Network残差网络。Kaiming He 的<a href="https://links.jianshu.com/go?to=https%3A%2F%2Farxiv.org%2Fabs%2F1512.03385" target="_blank" rel="noopener">《Deep Residual Learning for Image Recognition》</a>获得了CVPR最佳论文。<br>  随着网络深度的不断增加，人们发现深度CNN网络达到一定深度后再一味地增加层数并不能带来进一步地分类性能提高，反而会招致网络收敛变得更慢，test dataset的分类准确率也变得更差。</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/11753914-27655f0d9960084e.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200" alt="img"></p>
<p>  因此人们提出了残差网络。</p>
<h5 id="退化问题的原因"><a href="#退化问题的原因" class="headerlink" title="退化问题的原因"></a>退化问题的原因</h5><blockquote>
<p>  臭名昭著的梯度消失和梯度爆炸问题已经通过提出的标准初始化(如 Xavier, Msra)和中间层标准化(BN)解决. 退化问题也不是由过拟合造成, 因为在训练集上, 深层网络的性能就不如浅层网络。</p>
</blockquote>
<hr>
<h3 id="残差网络的提出"><a href="#残差网络的提出" class="headerlink" title="残差网络的提出"></a>残差网络的提出</h3><ul>
<li>神经网络地层数为什么如此重要？</li>
</ul>
<blockquote>
<p>因为CNN能够提取low/mid/high-level的特征，网络的层数越多，意味着能够提取到不同level的特征越丰富。并且，越深的网络提取的特征越抽象，越具有语义信息。</p>
</blockquote>
<ul>
<li>为什么不能简单地增加网络层数呢？<br>  因为简单地增加网络层数会导致梯度消失和梯度爆炸，因此，人们提出了正则化初始化和中间的正则化层(Batch Normalization)，但是 又引发了另外一个问题——退化问题，即随着网络层数地增加，训练集上的准确率却饱和甚至下降。这个问题并不是由过拟合(overfit)造成的，因为过拟合表现应该表现为在训练集上变现更好。</li>
<li>如何解决退化问题？<br>？？？未完成</li>
</ul>
<h3 id="残差网络的结构"><a href="#残差网络的结构" class="headerlink" title="残差网络的结构"></a>残差网络的结构</h3><h4 id="shortcut-connection"><a href="#shortcut-connection" class="headerlink" title="shortcut connection"></a>shortcut connection</h4><p>  ResNet的主要思想是在网络中增加了直连通道，即Highway Network的思想。此前的网络结构是性能输入做一个非线性变换，而Highway Network则允许保留之前网络层的一定比例的输出。ResNet的思想和Highway Network的思想也非常类似，允许原始输入信息直接传到后面的层中，如下图所示。</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/11753914-95c14f704c7fc204.png?imageMogr2/auto-orient/strip|imageView2/2/w/640" alt="img"></p>
<p>残差块</p>
<p>上图中的<br>  我们要求的映射是<img src="https://math.jianshu.com/math?formula=H(x" alt="H(x)">)。<br>  现在我们将这个问题改变为求解网络的残差映射函数，即<img src="https://math.jianshu.com/math?formula=F(x" alt="F(x)">)，<img src="https://math.jianshu.com/math?formula=F(x" alt="F(x) = H(x)-x">%20%3D%20H(x)-x)。</p>
<blockquote>
<p>残差：观测值与估计值之间的差。<br>这里H(x)就是观测值，x就是估计值（也就是上一层ResNet输出的特征映射）。<br>我们一般称x为identity Function，它是一个跳跃连接；称F(x)为ResNet Function。</p>
</blockquote>
<p>  所以我们要求解的问题转变为<img src="https://math.jianshu.com/math?formula=H(x" alt="H(x) = F(x) + x">%20%3D%20F(x)%20%2B%20x)。</p>
<blockquote>
<p>有小伙伴可能会疑惑，咱们干嘛非要经过F(x)之后在求解H(x)啊！整这么麻烦干嘛！<br>咱们开始看图说话：如果是采用一般的卷积神经网络的化，原先咱们要求解的是H(x) = F(x)这个值对不？那么，我们现在假设，在我的网络达到某一个深度的时候，咱们的网络已经达到最优状态了，也就是说，此时的错误率是最低的时候，再往下加深网络的化就会出现退化问题（错误率上升的问题）。咱们现在要更新下一层网络的权值就会变得很麻烦，权值得是一个让下一层网络同样也是最优状态才行。对吧？<br>但是采用残差网络就能很好的解决这个问题。还是假设当前网络的深度能够使得错误率最低，如果继续增加咱们的ResNet，为了保证下一层的网络状态仍然是最优状态，咱们只需要把令F(x)=0就好啦！因为x是当前输出的最优解，为了让它成为下一层的最优解也就是希望咱们的输出H(x)=x的话，是不是只要让F(x)=0就行了？<br>当然上面提到的只是理想情况，咱们在真实测试的时候x肯定是很难达到最优的，但是总会有那么一个时刻它能够无限接近最优解。采用ResNet的话，也只用小小的更新F(x)部分的权重值就行啦！不用像一般的卷积层一样大动干戈！</p>
</blockquote>
<h4 id="Basic-Block"><a href="#Basic-Block" class="headerlink" title="Basic Block"></a>Basic Block</h4><p><img src="https:////upload-images.jianshu.io/upload_images/11753914-ab6bad517b3b2ae4.png?imageMogr2/auto-orient/strip|imageView2/2/w/421" alt="img"></p>
<p>Basic Block（图源见水印）</p>
<p>  Basic Block主要针对ResNet18和ResNet34，basic block先做一个33的卷积，使用batchnorm，上图中旁边的就是所谓的”shortcut connection”,</p>
<h4 id="Basic-Block代码实现"><a href="#Basic-Block代码实现" class="headerlink" title="Basic Block代码实现"></a>Basic Block代码实现</h4><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicBlock</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line"></span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, in_channels, out_channels, stride=<span class="number">1</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>().__init_<span class="number">_</span>()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#residual function</span></span><br><span class="line">        <span class="keyword">self</span>.residual_function = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=False),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=True),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=False),</span><br><span class="line">            nn.BatchNorm2d(out_channels * BasicBlock.expansion)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">#shortcut</span></span><br><span class="line">        <span class="keyword">self</span>.shortcut = nn.Sequential()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#the shortcut output dimension is not the same with residual function</span></span><br><span class="line">        <span class="comment">#use 1*1 convolution to match the dimension</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> in_channels != BasicBlock.expansion * <span class="symbol">out_channels:</span></span><br><span class="line">            <span class="keyword">self</span>.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=<span class="number">1</span>, stride=stride, bias=False),</span><br><span class="line">                nn.BatchNorm2d(out_channels * BasicBlock.expansion)</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, x)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> nn.ReLU(inplace=True)(<span class="keyword">self</span>.residual_function(x) + <span class="keyword">self</span>.shortcut(x))</span><br></pre></td></tr></table></figure>
<p><img src="https:////upload-images.jianshu.io/upload_images/11753914-f07cf65c67b50bdd.png?imageMogr2/auto-orient/strip|imageView2/2/w/844" alt="img"></p>
<p>Basic Block</p>
<p>&amp;emsp: </p>
<h4 id="Bottleneck-Block"><a href="#Bottleneck-Block" class="headerlink" title="Bottleneck Block"></a>Bottleneck Block</h4><p><img src="https:////upload-images.jianshu.io/upload_images/11753914-2e266cfa660a8a74.png?imageMogr2/auto-orient/strip|imageView2/2/w/384" alt="img"></p>
<p>Bottleneck Block（图源见水印）</p>
<p>  上图中这种结构更为常用，用于resnet50，resnet101等。第一个1x1的卷积把256维channel降到64维，然后在最后通过1x1卷积恢复，中间做一次3*3的卷积操作，整体上相比于其他网络大大地降低了参数。</p>
<p><img src="https:////upload-images.jianshu.io/upload_images/11753914-4c3348bd881bd4e6.png?imageMogr2/auto-orient/strip|imageView2/2/w/1092" alt="img"></p>
<p>Basic Block和Bottleneck Block</p>
<ul>
<li>如果F(x)和x的channel个数不同怎么办，因为F(x)和x是按照channel维度相加的，channel不同怎么相加呢？<br>  <br>针对channel个数是否相同，要分成两种情况考虑:</li>
<li>shortcut connection为实线的时候，则channel一致，直接执行计算<img src="https://math.jianshu.com/math?formula=y%20%3D%20F(x" alt="y = F(x)+x">%2Bx)</li>
<li>shortcut connection为虚线的时候,采用的计算方式为<img src="https://math.jianshu.com/math?formula=y%20%3D%20F(x" alt="y = F(x)+Wx">%2BWx)，<img src="https://math.jianshu.com/math?formula=W" alt="W">为卷积操作，用来调整维度。</li>
</ul>
<blockquote>
<p>虚线残差连接为残差块输入输出维度不同. 当维度相同时, 使用恒等映射; 当输出维度升高(stride=2)时, 使用 zero-padding(使用全0填充缺少的维度, 然后concat低维数据从而升到高纬度) 或者 projection shortut(通过 1×1 卷积升维), 使用 projection shortut 效果略好一点点</p>
</blockquote>
<h4 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h4><p><img src="https:////upload-images.jianshu.io/upload_images/11753914-0c9b9b79e98c7731.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200" alt="img"></p>
<p>ResNet结构（图片来自于参考)</p>
<p>  从上图中可以看出，浅层网络和深层网络的主要差别主要在Conv4，深层网络有更多的层数。<br>  接下来我们以resnet18为例介绍resnet的网络结构。</p>
<p>  根据上面表格中的表述，可以将整个网络的主体部分分为4块<br>，conv2_x，conv3_x，conv4_x，conv5_x，在所有的结构中conv2_x这一部分都没有对的大小做改变feature map。而conv3_x，conv4_x，conv5_x则都将对feature map缩小两倍。</p>
<blockquote>
<p>在conv2_x中共有2个basicBlock，此部分的feature map输入（conv1的输出）channel为64，输出也是64，输入和输出的channel相同，且feature size也相同，因此可以直接相加，所以就不需要在shortcut中添加那个1*1的卷积了。</p>
</blockquote>
<blockquote>
<p>在conv3_x中也有2个basicBlock，此部分的输入channel是64，而输出channel是128，因此不匹配。而且，在这个模块中将feature size缩小了两倍，所以在conv3_x中的第一个basicBlock中，使用了stride=2，此时feature map的size也已经缩小了，所以需要在shortcut中添加1<em>1卷积，并且stride=2，让输入和输出具有相同的形状，从而可以相加。 在conv3_x的第二个block则不要shortcut中的1</em>1卷积，也没有出现stride=2的情况，因为这个block的输入和输出形状相同。</p>
</blockquote>
<blockquote>
<p>conv4_x、conv5_x和conv3_x的情况一模一样了。所以不做具体分析了。</p>
</blockquote>
<blockquote>
<p>总结:<br>ResNet中的每个conv模块（conv2_x，conv3_x，conv4_x，conv5_x）中，如果当前模块的输入channel以及feature map size与输出的不一样，那么会在当前模块的第一个block（basicBlock或bottleNeck）中使用stride=2将feature map size缩小，并且在shortcut中添加一个1<em>1的卷积，用来将输入channel和输出channel进行匹配。当前模块的所有非第一个block，都是采用正常的结构（stride=1， 并且shortcut中没有1</em>1卷积）</p>
</blockquote>
<p><img src="https:////upload-images.jianshu.io/upload_images/11753914-5ce18da629e4ae1a.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200" alt="img"></p>
<p>残差网络结构图</p>
<h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BasicBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Basic Block for resnet 18 and resnet 34</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#BasicBlock and BottleNeck block </span></span><br><span class="line">    <span class="comment">#have different output size</span></span><br><span class="line">    <span class="comment">#we use class attribute expansion</span></span><br><span class="line">    <span class="comment">#to distinct</span></span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#residual function</span></span><br><span class="line">        self.residual_function = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels * BasicBlock.expansion)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment">#shortcut</span></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#the shortcut output dimension is not the same with residual function</span></span><br><span class="line">        <span class="comment">#use 1*1 convolution to match the dimension</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> in_channels != BasicBlock.expansion * out_channels:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(out_channels * BasicBlock.expansion)</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> nn.ReLU(inplace=<span class="literal">True</span>)(self.residual_function(x) + self.shortcut(x))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BottleNeck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""Residual block for resnet over 50 layers</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, out_channels, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.residual_function = nn.Sequential(</span><br><span class="line">            nn.Conv2d(in_channels, out_channels, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(out_channels * BottleNeck.expansion),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> in_channels != out_channels * BottleNeck.expansion:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(out_channels * BottleNeck.expansion)</span><br><span class="line">            )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> nn.ReLU(inplace=<span class="literal">True</span>)(self.residual_function(x) + self.shortcut(x))</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, block, num_block, num_classes=<span class="number">100</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        self.in_channels = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>))</span><br><span class="line">        <span class="comment">#we use a different inputsize than the original paper</span></span><br><span class="line">        <span class="comment">#so conv2_x's stride is 1</span></span><br><span class="line">        self.conv2_x = self._make_layer(block, <span class="number">64</span>, num_block[<span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">        self.conv3_x = self._make_layer(block, <span class="number">128</span>, num_block[<span class="number">1</span>], <span class="number">2</span>)</span><br><span class="line">        self.conv4_x = self._make_layer(block, <span class="number">256</span>, num_block[<span class="number">2</span>], <span class="number">2</span>)</span><br><span class="line">        self.conv5_x = self._make_layer(block, <span class="number">512</span>, num_block[<span class="number">3</span>], <span class="number">2</span>)</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self, block, out_channels, num_blocks, stride)</span>:</span></span><br><span class="line">        <span class="string">"""make resnet layers(by layer i didnt mean this 'layer' was the </span></span><br><span class="line"><span class="string">        same as a neuron netowork layer, ex. conv layer), one layer may </span></span><br><span class="line"><span class="string">        contain more than one residual block </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            block: block type, basic block or bottle neck block</span></span><br><span class="line"><span class="string">            out_channels: output depth channel number of this layer</span></span><br><span class="line"><span class="string">            num_blocks: how many blocks per layer</span></span><br><span class="line"><span class="string">            stride: the stride of the first block of this layer</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Return:</span></span><br><span class="line"><span class="string">            return a resnet layer</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># we have num_block blocks per layer, the first block </span></span><br><span class="line">        <span class="comment"># could be 1 or 2, other blocks would always be 1</span></span><br><span class="line">        strides = [stride] + [<span class="number">1</span>] * (num_blocks - <span class="number">1</span>)</span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> stride <span class="keyword">in</span> strides:</span><br><span class="line">            layers.append(block(self.in_channels, out_channels, stride))</span><br><span class="line">            self.in_channels = out_channels * block.expansion</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        output = self.conv1(x)</span><br><span class="line">        output = self.conv2_x(output)</span><br><span class="line">        output = self.conv3_x(output)</span><br><span class="line">        output = self.conv4_x(output)</span><br><span class="line">        output = self.conv5_x(output)</span><br><span class="line">        output = self.avg_pool(output)</span><br><span class="line">        output = output.view(output.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        output = self.fc(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet18</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" return a ResNet 18 object</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(BasicBlock, [<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet34</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" return a ResNet 34 object</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet50</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" return a ResNet 50 object</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(BottleNeck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet101</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" return a ResNet 101 object</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(BottleNeck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">resnet152</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">""" return a ResNet 152 object</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(BottleNeck, [<span class="number">3</span>, <span class="number">8</span>, <span class="number">36</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<h3 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h3><p><a href="https://links.jianshu.com/go?to=https%3A%2F%2Fwww.cnblogs.com%2Fnowgood%2Fp%2FResNet.html" target="_blank" rel="noopener">resnet</a><br><a href="https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fcsdnldp%2Farticle%2Fdetails%2F78313087" target="_blank" rel="noopener">ResNet详细解读</a><br><a href="https://links.jianshu.com/go?to=https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F48169294" target="_blank" rel="noopener">ResNet结构分析</a><br><a href="https://links.jianshu.com/go?to=https%3A%2F%2Fblog.csdn.net%2Fu013181595%2Farticle%2Fdetails%2F80990930" target="_blank" rel="noopener">ResNet介绍</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/残差网络/" rel="tag"># 残差网络</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/12/17/ResNets/" rel="next" title="ResNets">
                <i class="fa fa-chevron-left"></i> ResNets
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/12/30/text4/" rel="prev" title>
                 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/uploads/logo.jpg" alt="xiaohaoke">
            
              <p class="site-author-name" itemprop="name">xiaohaoke</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#残差网络"><span class="nav-number">1.</span> <span class="nav-text">残差网络</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#退化问题的原因"><span class="nav-number">1.0.0.1.</span> <span class="nav-text">退化问题的原因</span></a></li></ol></li></ol><li class="nav-item nav-level-3"><a class="nav-link" href="#残差网络的提出"><span class="nav-number">1.1.</span> <span class="nav-text">残差网络的提出</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#残差网络的结构"><span class="nav-number">1.2.</span> <span class="nav-text">残差网络的结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#shortcut-connection"><span class="nav-number">1.2.1.</span> <span class="nav-text">shortcut connection</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Basic-Block"><span class="nav-number">1.2.2.</span> <span class="nav-text">Basic Block</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Basic-Block代码实现"><span class="nav-number">1.2.3.</span> <span class="nav-text">Basic Block代码实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bottleneck-Block"><span class="nav-number">1.2.4.</span> <span class="nav-text">Bottleneck Block</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#网络结构"><span class="nav-number">1.2.5.</span> <span class="nav-text">网络结构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码实现"><span class="nav-number">1.3.</span> <span class="nav-text">代码实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参考："><span class="nav-number">1.4.</span> <span class="nav-text">参考：</span></a></li></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2015 &mdash; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xiaohaoke</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://xiaohaoke.top/2019/12/17/ResNet/';
          this.page.identifier = '2019/12/17/ResNet/';
          this.page.title = 'ResNet残差网络';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
